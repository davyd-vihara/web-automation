import pdfplumber
from collections import defaultdict
from dataclasses import dataclass, asdict
from typing import List, Tuple, Optional, Dict, Any
import colorsys
import os
from datetime import datetime
import argparse
import json
import re


@dataclass
class AccessibilityIssue:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏"""
    page: int
    x: float
    y: float
    text: str
    issue_type: str
    description: str
    severity: str  # 'low', 'medium', 'high'
    font_name: str = ""
    font_size: float = 0.0
    color: Tuple[float, float, float] = (0, 0, 0)  # RGB
    background_color: Tuple[float, float, float] = (1, 1, 1)  # RGB


class EnhancedPDFAccessibilityAnalyzer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ PDF —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è WCAG 2.1
    MIN_FONT_SIZE = 12  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    MIN_HEADING_SIZE = 14  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤

    # –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å (WCAG AA —É—Ä–æ–≤–µ–Ω—å)
    MIN_CONTRAST_RATIO = 4.5  # –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    MIN_CONTRAST_LARGE = 3.0  # –¥–ª—è –∫—Ä—É–ø–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (18pt+ –∏–ª–∏ 14pt –∂–∏—Ä–Ω—ã–π)

    # –•–æ—Ä–æ—à–æ —á–∏—Ç–∞–µ–º—ã–µ —à—Ä–∏—Ñ—Ç—ã –¥–ª—è —Å–ª–∞–±–æ–≤–∏–¥—è—â–∏—Ö
    ACCESSIBLE_FONTS = {
        'Arial', 'Helvetica', 'Verdana', 'Tahoma', 'Calibri',
        'Georgia', 'Times New Roman', 'Lucida Sans', 'Trebuchet MS',
        'Open Sans', 'Roboto', 'Lato', 'Montserrat',
        'LiberationSans', 'LiberationSerif', 'DejaVu Sans', 'DejaVu Serif'
    }

    # –ü–ª–æ—Ö–æ —á–∏—Ç–∞–µ–º—ã–µ —à—Ä–∏—Ñ—Ç—ã (–¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ, –º–æ–Ω–æ—à–∏—Ä–∏–Ω–Ω—ã–µ –∏ —Ç.–¥.)
    POOR_READABILITY_FONTS = {
        'Comic Sans', 'Papyrus', 'Brush Script', 'Jokerman',
        'Chiller', 'Curly', 'Old English', 'Gothic',
        'Courier', 'Consolas', 'Monaco', 'Menlo', 'Source Code Pro'
    }

    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.issues: List[AccessibilityIssue] = []
        self.color_issues: List[dict] = []  # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º —Å —Ü–≤–µ—Ç–∞–º–∏
        self.line_cache = {}  # –ö—ç—à –¥–ª—è —Å—Ç—Ä–æ–∫ —Ç–µ–∫—Å—Ç–∞
        self.problematic_colors_found = []  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
        self.full_text_cache = {}  # –ö—ç—à –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç—Ä–æ–∫
        self.screenshots_dir = "accessibility_screenshots"

    def normalize_color(self, color) -> Tuple[float, float, float]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ü–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç RGB (0-1)"""
        try:
            if isinstance(color, (int, float)):
                # –ú–æ–Ω–æ—Ö—Ä–æ–º–Ω—ã–π —Ü–≤–µ—Ç (grayscale)
                return (float(color), float(color), float(color))
            elif isinstance(color, tuple) or isinstance(color, list):
                if len(color) == 1:
                    # –ú–æ–Ω–æ—Ö—Ä–æ–º–Ω—ã–π
                    return (float(color[0]), float(color[0]), float(color[0]))
                elif len(color) == 3:
                    # RGB
                    return (float(color[0]), float(color[1]), float(color[2]))
                elif len(color) == 4:
                    # CMYK - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                    c, m, y, k = color
                    r = (1 - c) * (1 - k)
                    g = (1 - m) * (1 - k)
                    b = (1 - y) * (1 - k)
                    return (r, g, b)
            elif color is None:
                # –¶–≤–µ—Ç –Ω–µ —É–∫–∞–∑–∞–Ω - –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á–µ—Ä–Ω—ã–π
                return (0.0, 0.0, 0.0)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ü–≤–µ—Ç–∞ {color}: {e}")

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —á–µ—Ä–Ω—ã–π
        return (0.0, 0.0, 0.0)

    def is_large_text_by_wcag(self, font_size: float, font_name: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∫—Ä—É–ø–Ω—ã–º –ø–æ WCAG 2.1
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏:
        - –†–∞–∑–º–µ—Ä ‚â• 18pt
        –ò–õ–ò
        - –†–∞–∑–º–µ—Ä ‚â• 14pt –ò —Ç–µ–∫—Å—Ç –∂–∏—Ä–Ω—ã–π
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —à—Ä–∏—Ñ—Ç –∂–∏—Ä–Ω—ã–º
            is_bold = any(bold_term in font_name for bold_term in
                          ['Bold', 'BoldItalic', 'Black', 'Heavy', '-Bold', 'bold'])

            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ WCAG
            if font_size >= 18:
                return True  # ‚â•18pt - –≤—Å–µ–≥–¥–∞ –∫—Ä—É–ø–Ω—ã–π
            elif font_size >= 14 and is_bold:
                return True  # ‚â•14pt –ò –∂–∏—Ä–Ω—ã–π - –∫—Ä—É–ø–Ω—ã–π
            else:
                return False  # –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        except:
            return False

    def calculate_luminance(self, color: Tuple[float, float, float]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —è—Ä–∫–æ—Å—Ç—å —Ü–≤–µ—Ç–∞ (0-1)"""
        try:
            r, g, b = color

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ sRGB –≤ –ª–∏–Ω–µ–π–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            def srgb_to_linear(channel):
                if channel <= 0.03928:
                    return channel / 12.92
                return ((channel + 0.055) / 1.055) ** 2.4

            r_linear = srgb_to_linear(r)
            g_linear = srgb_to_linear(g)
            b_linear = srgb_to_linear(b)

            # –†–∞—Å—Å—á–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–π —è—Ä–∫–æ—Å—Ç–∏
            return 0.2126 * r_linear + 0.7152 * g_linear + 0.0722 * b_linear
        except:
            return 0.0

    def calculate_contrast_ratio(self, color1: Tuple[float, float, float],
                                 color2: Tuple[float, float, float]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ü–≤–µ—Ç–∞–º–∏"""
        try:
            l1 = self.calculate_luminance(color1)
            l2 = self.calculate_luminance(color2)

            # –ë–æ–ª–µ–µ —Å–≤–µ—Ç–ª—ã–π –∏ —Ç–µ–º–Ω—ã–π —Ü–≤–µ—Ç–∞
            lighter = max(l1, l2)
            darker = min(l1, l2)

            return (lighter + 0.05) / (darker + 0.05)
        except:
            return 1.0  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ

    def rgb_to_hsv(self, color: Tuple[float, float, float]) -> Tuple[float, float, float]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç RGB –≤ HSV —Ü–≤–µ—Ç–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ"""
        try:
            return colorsys.rgb_to_hsv(color[0], color[1], color[2])
        except:
            return (0.0, 0.0, 0.0)

    def identify_problematic_color(self, color: Tuple[float, float, float]) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ü–≤–µ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã–º –¥–ª—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ HSV –¥–ª—è –ª—É—á—à–µ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
            h, s, v = self.rgb_to_hsv(color)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç –ø–æ Hue
            if 0.2 <= h <= 0.4:  # –ó–µ–ª–µ–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
                if s > 0.3 and v > 0.4:
                    if v > 0.7:
                        return "—Å–≤–µ—Ç–ª–æ-–∑–µ–ª–µ–Ω—ã–π"
                    else:
                        return "–∑–µ–ª–µ–Ω—ã–π"
            elif h <= 0.05 or h >= 0.95:  # –ö—Ä–∞—Å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
                if s > 0.3 and v > 0.4:
                    if v > 0.7:
                        return "—Å–≤–µ—Ç–ª–æ-–∫—Ä–∞—Å–Ω—ã–π"
                    else:
                        return "–∫—Ä–∞—Å–Ω—ã–π"
            elif 0.55 <= h <= 0.75:  # –°–∏–Ω–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω
                if s > 0.3 and v > 0.4:
                    if v > 0.7:
                        return "—Å–≤–µ—Ç–ª–æ-—Å–∏–Ω–∏–π"
                    else:
                        return "—Å–∏–Ω–∏–π"
            elif 0.05 <= h <= 0.15:  # –ñ–µ–ª—Ç—ã–π/–æ—Ä–∞–Ω–∂–µ–≤—ã–π
                if s > 0.3 and v > 0.6:
                    return "–∂–µ–ª—Ç—ã–π" if v > 0.8 else "–æ—Ä–∞–Ω–∂–µ–≤—ã–π"

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ—Ä—ã–µ –æ—Ç—Ç–µ–Ω–∫–∏
            if s < 0.1 and 0.3 <= v <= 0.7:
                return "—Å–µ—Ä—ã–π"

            return None
        except:
            return None

    def normalize_font_name(self, font_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —à—Ä–∏—Ñ—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        try:
            if '+' in font_name:
                font_name = font_name.split('+')[-1]

            font_name = font_name.replace('-Bold', '').replace('-Italic', '')
            font_name = font_name.replace('Bold', '').replace('Italic', '')
            font_name = font_name.replace('MT', '').replace('PS', '')

            return font_name.strip()
        except:
            return font_name

    def check_font_readability(self, font_name: str) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —à—Ä–∏—Ñ—Ç –∫ —Ö–æ—Ä–æ—à–æ —á–∏—Ç–∞–µ–º—ã–º"""
        try:
            normalized_name = self.normalize_font_name(font_name)

            for accessible_font in self.ACCESSIBLE_FONTS:
                if accessible_font.lower() in normalized_name.lower():
                    return True, f"–•–æ—Ä–æ—à–æ —á–∏—Ç–∞–µ–º—ã–π —à—Ä–∏—Ñ—Ç: {accessible_font}"

            for poor_font in self.POOR_READABILITY_FONTS:
                if poor_font.lower() in normalized_name.lower():
                    return False, f"–ü–ª–æ—Ö–æ —á–∏—Ç–∞–µ–º—ã–π —à—Ä–∏—Ñ—Ç: {poor_font}"

            return False, f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —à—Ä–∏—Ñ—Ç: {normalized_name}"
        except:
            return True, "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —à—Ä–∏—Ñ—Ç"

    def extract_background_color(self, page, x: float, y: float) -> Tuple[float, float, float]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–≤–µ—Ç–∞ —Ñ–æ–Ω–∞"""
        return (1.0, 1.0, 1.0)  # –±–µ–ª—ã–π —Ñ–æ–Ω

    def remove_duplicate_chars(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text or len(text) < 2:
            return text

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (2 –∏–ª–∏ –±–æ–ª–µ–µ –ø–æ–¥—Ä—è–¥)

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        result = []
        i = 0
        while i < len(text):
            result.append(text[i])
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã
            j = i + 1
            while j < len(text) and text[j] == text[i]:
                j += 1
            i = j

        cleaned = ''.join(result)

        # –¢–∞–∫–∂–µ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–µ 2 —Å–∏–º–≤–æ–ª–∞
        # (—Ç–∏–ø–∞ "–ì–õ–û–û–°–°–ê–ê–†–†–ò–ò–ô–ô")
        # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω–æ —Å –∫–µ—Ä–Ω–∏–Ω–≥–æ–º/–Ω–∞–ª–æ–∂–µ–Ω–∏–µ–º –≤ PDF
        if len(cleaned) % 2 == 0:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ —Å–∏–º–≤–æ–ª
            half = len(cleaned) // 2
            first_half = cleaned[:half]
            second_half = cleaned[half:]

            # –ï—Å–ª–∏ –≤—Ç–æ—Ä–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –ø–µ—Ä–≤—É—é —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
            if first_half == second_half:
                # –ë–µ—Ä–µ–º –∫–∞–∂–¥—ã–π –≤—Ç–æ—Ä–æ–π —Å–∏–º–≤–æ–ª –∏–∑ –ø–µ—Ä–≤–æ–π –ø–æ–ª–æ–≤–∏–Ω—ã
                return first_half

        return cleaned

    def count_words(self, text: str) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text or not text.strip():
            return 0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç - —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
        normalized = self.remove_duplicate_chars(text.strip())
        if not normalized:
            return 0
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: –ø—Ä–æ–±–µ–ª—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞
        words = re.findall(r'\b\w+\b', normalized, re.UNICODE)
        return len(words)

    def normalize_text_for_grouping(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
        if not text:
            return text

        # 1. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text = self.remove_duplicate_chars(text)

        # 2. –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = ' '.join(text.split()).lower()

        # 3. –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ –¥–ª—è –ª—É—á—à–µ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        text = text.rstrip('.,;:!?')

        return text

    def get_text_line(self, page, y_position: float, tolerance: float = 2.0) -> Tuple[List[dict], str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –≤ —Å—Ç—Ä–æ–∫–µ –ø–æ Y-–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ –∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫–∏"""
        try:
            cache_key = (id(page), round(y_position, 2))

            if cache_key not in self.line_cache:
                line_chars = []
                for char in page.chars:
                    if abs(char['y0'] - y_position) < tolerance:
                        line_chars.append(char)

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ X –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–µ
                line_chars.sort(key=lambda c: c['x0'])

                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫–∏
                line_text = ''.join([c.get('text', '') for c in line_chars])

                self.line_cache[cache_key] = (line_chars, line_text)
                self.full_text_cache[cache_key] = line_text

            return self.line_cache[cache_key]
        except:
            return ([], "")

    def analyze_text_line_contrast(self, page_num: int, line_chars: List[dict], line_text: str) -> List[
        AccessibilityIssue]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å —Ü–µ–ª–æ–π —Å—Ç—Ä–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        issues = []

        if not line_chars or not line_text.strip():
            return issues

        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç - —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
            normalized_line_text = self.remove_duplicate_chars(line_text.strip())
            if not normalized_line_text or len(normalized_line_text) < 3:
                return issues

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å—Ç—Ä–æ–∫–∏
            avg_size = sum(char.get('size', 12) for char in line_chars) / len(line_chars)
            is_bold = any('Bold' in char.get('fontname', '') for char in line_chars)
            is_large_wcag = self.is_large_text_by_wcag(avg_size, line_chars[0].get('fontname', ''))

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–µ–±—É–µ–º—É—é –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å
            required_contrast = self.MIN_CONTRAST_LARGE if is_large_wcag else self.MIN_CONTRAST_RATIO

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å–∏–º–≤–æ–ª
            for char in line_chars:
                try:
                    raw_color = char.get('non_stroking_color', (0, 0, 0))
                    text_color = self.normalize_color(raw_color)

                    # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ü–≤–µ—Ç–∞
                    if raw_color not in [(0, 0, 0), 0, None, (0,), [0]]:
                        self.problematic_colors_found.append({
                            'page': page_num,
                            'color': raw_color,
                            'normalized': text_color,
                            'text': char.get('text', '')
                        })

                    bg_color = self.extract_background_color(None, char.get('x0', 0), char.get('y0', 0))
                    contrast_ratio = self.calculate_contrast_ratio(text_color, bg_color)

                    if contrast_ratio < required_contrast:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π —Ü–≤–µ—Ç
                        color_name = self.identify_problematic_color(text_color)

                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å
                        if contrast_ratio < 2.0:
                            severity = 'high'
                        elif contrast_ratio < 3.0:
                            severity = 'medium'
                        else:
                            severity = 'low'

                        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
                        if is_large_wcag:
                            size_info = f"–ö—Ä—É–ø–Ω—ã–π —Ç–µ–∫—Å—Ç ({avg_size:.1f}pt{' –∂–∏—Ä–Ω—ã–π' if is_bold else ''})"
                            contrast_req = f"—Ç—Ä–µ–±—É–µ—Ç—Å—è ‚â•3.0:1"
                        else:
                            size_info = f"–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç ({avg_size:.1f}pt)"
                            contrast_req = f"—Ç—Ä–µ–±—É–µ—Ç—Å—è ‚â•4.5:1"

                        issue_desc = f"{size_info}. –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å: {contrast_ratio:.1f}:1 ({contrast_req})"
                        if color_name:
                            issue_desc += f". –ü—Ä–æ–±–ª–µ–º–Ω—ã–π —Ü–≤–µ—Ç: {color_name}"

                        # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ)
                        text_preview = normalized_line_text
                        if len(text_preview) > 150:
                            text_preview = text_preview[:147] + "..."

                        issues.append(AccessibilityIssue(
                            page=page_num,
                            x=char.get('x0', 0),
                            y=char.get('y0', 0),
                            text=text_preview,
                            issue_type='–ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å',
                            description=issue_desc,
                            severity=severity,
                            font_name=char.get('fontname', ''),
                            font_size=char.get('size', 12),
                            color=text_color,
                            background_color=bg_color
                        ))

                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ü–≤–µ—Ç–æ–≤ (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º)
                        if color_name and contrast_ratio < 4.5:
                            self.color_issues.append({
                                'page': page_num,
                                'raw_color': raw_color,
                                'color': text_color,
                                'color_name': color_name,
                                'contrast': contrast_ratio,
                                'required': required_contrast,
                                'text_sample': normalized_line_text[:100].strip(),
                                'full_text': normalized_line_text.strip(),
                                'position': (char.get('x0', 0), char.get('y0', 0)),
                                'is_large': is_large_wcag,
                                'font_size': char.get('size', 12)
                            })
                except Exception:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã

        except Exception:
            pass  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏

        return issues

    def analyze_page(self, page_num: int, page) -> List[AccessibilityIssue]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏"""
        page_issues = []
        processed_lines = set()

        try:
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Å–∏–º–≤–æ–ª–∞–º
            for char in page.chars:
                try:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–µ–ø–µ—á–∞—Ç–∞–µ–º—ã–µ —Å–∏–º–≤–æ–ª—ã
                    char_text = char.get('text', '')
                    if char_text.isspace() or not char_text.strip():
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏
                    line_y = round(char.get('y0', 0), 2)
                    if line_y not in processed_lines:
                        line_chars, line_text = self.get_text_line(page, char.get('y0', 0))

                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å —Å—Ç—Ä–æ–∫–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç)
                        if line_text.strip():
                            contrast_issues = self.analyze_text_line_contrast(page_num, line_chars, line_text)
                            page_issues.extend(contrast_issues)

                        processed_lines.add(line_y)

                    # 2. –ü–†–û–í–ï–†–ö–ê –†–ê–ó–ú–ï–†–ê –®–†–ò–§–¢–ê (–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–∞—è)
                    font_size = char.get('size', 12)
                    font_name = char.get('fontname', '')
                    is_bold = 'Bold' in font_name
                    is_large_wcag = self.is_large_text_by_wcag(font_size, font_name)

                    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ç–µ–∫—Å—Ç–∞
                    line_chars, line_text = self.get_text_line(page, char.get('y0', 0))
                    normalized_text = self.remove_duplicate_chars(line_text.strip())

                    if not normalized_text or len(normalized_text) < 3:
                        continue

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–µ–∫—Å—Ç–∞
                    if is_bold and font_size >= 14:
                        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                        if font_size < self.MIN_HEADING_SIZE:
                            text_preview = normalized_text[:80] + ("..." if len(normalized_text) > 80 else "")

                            page_issues.append(AccessibilityIssue(
                                page=page_num,
                                x=char.get('x0', 0),
                                y=char.get('y0', 0),
                                text=text_preview,
                                issue_type='–†–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞',
                                description=f'–†–∞–∑–º–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–∞ {font_size:.1f}pt –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ {self.MIN_HEADING_SIZE}pt',
                                severity='high',
                                font_name=font_name,
                                font_size=font_size
                            ))
                    elif not is_large_wcag:  # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç (–Ω–µ –∫—Ä—É–ø–Ω—ã–π –ø–æ WCAG)
                        if font_size < self.MIN_FONT_SIZE:
                            text_preview = normalized_text[:80] + ("..." if len(normalized_text) > 80 else "")

                            page_issues.append(AccessibilityIssue(
                                page=page_num,
                                x=char.get('x0', 0),
                                y=char.get('y0', 0),
                                text=text_preview,
                                issue_type='–†–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞',
                                description=f'–†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ {font_size:.1f}pt –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ {self.MIN_FONT_SIZE}pt',
                                severity='medium' if font_size >= 10 else 'high',
                                font_name=font_name,
                                font_size=font_size
                            ))

                    # 3. –ü–†–û–í–ï–†–ö–ê –ß–ò–¢–ê–ï–ú–û–°–¢–ò –®–†–ò–§–¢–ê
                    is_readable, readability_info = self.check_font_readability(font_name)

                    if not is_readable:
                        text_preview = normalized_text[:80] + ("..." if len(normalized_text) > 80 else "")

                        page_issues.append(AccessibilityIssue(
                            page=page_num,
                            x=char.get('x0', 0),
                            y=char.get('y0', 0),
                            text=text_preview,
                            issue_type='–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å —à—Ä–∏—Ñ—Ç–∞',
                            description=readability_info,
                            severity='medium',
                            font_name=font_name,
                            font_size=font_size
                        ))
                except:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")

        return page_issues

    def group_and_summarize_issues_improved(self) -> Dict[str, Any]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å —á–µ—Ç–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏"""

        summary = {
            'by_type_severity': defaultdict(lambda: {
                'high': {'places': 0, 'words': 0},
                'medium': {'places': 0, 'words': 0},
                'low': {'places': 0, 'words': 0}
            }),
            'by_type': defaultdict(lambda: {
                'total_places': 0,
                'total_words': 0,
                'pages_affected': set()
            }),
            'by_severity': defaultdict(lambda: {
                'places': 0,
                'words': 0,
                'types': defaultdict(lambda: {'places': 0, 'words': 0})
            }),
            'overall': {
                'total_places': 0,
                'total_words': 0,
                'pages_with_issues': set(),
                'types_distribution': defaultdict(int),
                'severity_distribution': defaultdict(int)
            }
        }

        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º –ø—Ä–æ–±–ª–µ–º–∞–º
        for issue in self.issues:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ –ø—Ä–æ–±–ª–µ–º—ã
            word_count = self.count_words(issue.text)
            
            # 1. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø—É + —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å (–≤–ª–æ–∂–µ–Ω–Ω–∞—è)
            type_sev_group = summary['by_type_severity'][issue.issue_type]
            type_sev_group[issue.severity]['places'] += 1
            type_sev_group[issue.severity]['words'] += word_count

            # 2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø—É (—Å—É–º–º–∞—Ä–Ω–æ)
            type_group = summary['by_type'][issue.issue_type]
            type_group['total_places'] += 1
            type_group['total_words'] += word_count
            type_group['pages_affected'].add(issue.page)

            # 3. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            severity_group = summary['by_severity'][issue.severity]
            severity_group['places'] += 1
            severity_group['words'] += word_count
            severity_group['types'][issue.issue_type]['places'] += 1
            severity_group['types'][issue.issue_type]['words'] += word_count

            # 4. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            summary['overall']['total_places'] += 1
            summary['overall']['total_words'] += word_count
            summary['overall']['pages_with_issues'].add(issue.page)
            summary['overall']['types_distribution'][issue.issue_type] += 1
            summary['overall']['severity_distribution'][issue.severity] += 1

        return summary

    def create_screenshot(self, page_num: int, bbox: Tuple[float, float, float, float] = None,
                          issue_type: str = None, output_dir: str = None,
                          full_page: bool = False, highlight_issue: bool = False,
                          issue_position: Tuple[float, float] = None) -> Optional[str]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ –æ–±–ª–∞—Å—Ç–∏

        Args:
            page_num: –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            bbox: bounding box (x0, y0, x1, y1) –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
            issue_type: —Ç–∏–ø –ø—Ä–æ–±–ª–µ–º—ã
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            full_page: –µ—Å–ª–∏ True, —Å–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            highlight_issue: –µ—Å–ª–∏ True, –≤—ã–¥–µ–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—É—é –æ–±–ª–∞—Å—Ç—å
            issue_position: –ø–æ–∑–∏—Ü–∏—è –ø—Ä–æ–±–ª–µ–º—ã (x, y) –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è

        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Å–∫—Ä–∏–Ω—à–æ—Ç—É –∏–ª–∏ None
        """
        try:
            if output_dir is None:
                output_dir = self.screenshots_dir

            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            os.makedirs(output_dir, exist_ok=True)

            with pdfplumber.open(self.pdf_path) as pdf:
                if page_num > len(pdf.pages):
                    return None

                page = pdf.pages[page_num - 1]

                if full_page:
                    # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    im = page.to_image(resolution=150)
                    screenshot_type = "full_page"
                else:
                    if bbox is None:
                        # –ï—Å–ª–∏ bbox –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        bbox = (0, 0, page.width, page.height)
                        screenshot_type = "full_page"
                    else:
                        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—Ç—É–ø—ã –≤–æ–∫—Ä—É–≥ –ø—Ä–æ–±–ª–µ–º–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏
                        padding = 50 if highlight_issue else 20
                        x0, y0, x1, y1 = bbox
                        x0 = max(0, x0 - padding)
                        y0 = max(0, y0 - padding)
                        x1 = min(page.width, x1 + padding)
                        y1 = min(page.height, y1 + padding)

                        # –í—ã—Ä–µ–∑–∞–µ–º –æ–±–ª–∞—Å—Ç—å
                        cropped_page = page.crop((x0, y0, x1, y1))
                        im = cropped_page.to_image(resolution=150)
                        screenshot_type = "area"

                # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—É—é –æ–±–ª–∞—Å—Ç—å
                if highlight_issue and issue_position:
                    try:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è
                        if not full_page:
                            # –î–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞
                            x, y = issue_position
                            rel_x = x - bbox[0] if bbox else x
                            rel_y = y - bbox[1] if bbox else y

                            # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–¥–µ–ª–µ–Ω–∏–µ (–∫—Ä–∞—Å–Ω—ã–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫)
                            im.draw_rect((rel_x - 10, rel_y - 5, rel_x + 100, rel_y + 10),
                                         fill=None, stroke="red", stroke_width=3)

                            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å —Ç–∏–ø–æ–º –ø—Ä–æ–±–ª–µ–º—ã
                            if issue_type:
                                im.draw_text((rel_x, rel_y - 20), issue_type,
                                             fill="red", font_size=12)
                    except Exception as e:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–¥–µ–ª–µ–Ω–∏–∏ –ø—Ä–æ–±–ª–µ–º—ã: {e}")

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                if issue_type:
                    filename = f"page_{page_num}_{issue_type}_{screenshot_type}_{timestamp}.png"
                else:
                    filename = f"page_{page_num}_{screenshot_type}_{timestamp}.png"
                filepath = os.path.join(output_dir, filename)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                im.save(filepath, format="PNG", quality=95)

                print(f"üì∏ –°–∫—Ä–∏–Ω—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")
                return filepath

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞: {e}")
            return None

    def create_full_page_screenshots(self, pages: List[int] = None, output_dir: str = None) -> Dict[int, str]:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç–∞–Ω–∏—á–Ω—ã–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü

        Args:
            pages: —Å–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏)
            output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –Ω–æ–º–µ—Ä–∞–º–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ø—É—Ç—è–º–∏ –∫ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º
        """
        screenshots = {}

        if output_dir is None:
            output_dir = os.path.join(self.screenshots_dir, "full_pages")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –¥–ª—è –∫–∞–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–µ–ª–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
        if pages is None:
            # –°–æ–∑–¥–∞–µ–º —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏
            pages_with_issues = set(issue.page for issue in self.issues)
            pages = sorted(pages_with_issues)

        print(f"\nüì∏ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç–∞–Ω–∏—á–Ω—ã—Ö —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –¥–ª—è {len(pages)} —Å—Ç—Ä–∞–Ω–∏—Ü...")

        for page_num in pages:
            print(f"  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}...", end='\r')

            screenshot_path = self.create_screenshot(
                page_num=page_num,
                full_page=True,
                output_dir=output_dir
            )

            if screenshot_path:
                screenshots[page_num] = screenshot_path

        print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(screenshots)} –ø–æ–ª–Ω–æ—Å—Ç–∞–Ω–∏—á–Ω—ã—Ö —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤")
        return screenshots

    def truncate_text_smart(self, text: str, max_length: int = 50) -> str:
        """–£–º–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ —Å–ª–æ–≤–∞"""
        if len(text) <= max_length:
            return text

        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–æ–π
        truncated = text[:max_length]
        last_space = truncated.rfind(' ')

        if last_space > max_length * 0.7:  # –ï—Å–ª–∏ –ø—Ä–æ–±–µ–ª –µ—Å—Ç—å –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ç—Ä–µ—Ç–∏
            return truncated[:last_space] + "..."
        else:
            return truncated + "..."

    def group_issues_by_text_and_page(self, issues: List[AccessibilityIssue]) -> Dict[str, Dict[str, Any]]:
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É –∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {—Ç–µ–∫—Å—Ç: {—Å—Ç—Ä–∞–Ω–∏—Ü—ã: {—Å—Ç—Ä–∞–Ω–∏—Ü–∞: [–ø—Ä–æ–±–ª–µ–º—ã]}, count: X, first_issue: –ø—Ä–æ–±–ª–µ–º–∞}}
        """
        text_groups = defaultdict(lambda: {
            'pages': defaultdict(list),
            'total_count': 0,
            'first_issue': None,
            'issue_types': set(),
            'descriptions': set()
        })

        for issue in issues:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç (—É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
            normalized_text = self.normalize_text_for_grouping(issue.text)

            if len(normalized_text) < 3:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                continue

            group = text_groups[normalized_text]
            group['pages'][issue.page].append(issue)
            group['total_count'] += 1
            group['issue_types'].add(issue.issue_type)
            group['descriptions'].add(issue.description)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –æ—Ç—á–µ—Ç–∞
            if group['first_issue'] is None:
                group['first_issue'] = issue

        return text_groups

    def group_issues_by_text_pattern(self, issues: List[AccessibilityIssue]) -> Dict[str, Dict[str, Any]]:
        """
        –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: {—Ç–µ–∫—Å—Ç_–ø–∞—Ç—Ç–µ—Ä–Ω: {—Å—Ç—Ä–∞–Ω–∏—Ü—ã: [], –ø—Ä–æ–±–ª–µ–º—ã: [], —Å–∏–º–≤–æ–ª—ã: 0, —Ç–∏–ø—ã: set()}}
        """
        text_groups = defaultdict(lambda: {
            'pages': set(),
            'issues': [],
            'total_words': 0,
            'issue_types': set(),
            'severities': defaultdict(int),
            'descriptions': set(),
            'font_info': set()
        })

        for issue in issues:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ - —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
            text_key = self.normalize_text_for_grouping(issue.text)

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
            if len(text_key) < 5:
                continue

            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, –æ–±—Ä–µ–∑–∞–µ–º
            if len(text_key) > 50:
                text_key = self.truncate_text_smart(text_key, 50)

            group = text_groups[text_key]
            group['pages'].add(issue.page)
            group['issues'].append(issue)
            group['total_words'] += self.count_words(text_key)  # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞
            group['issue_types'].add(issue.issue_type)
            group['severities'][issue.severity] += 1
            group['descriptions'].add(issue.description[:100])  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –æ–ø–∏—Å–∞–Ω–∏—è

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —à—Ä–∏—Ñ—Ç–µ
            if issue.font_name and issue.font_size:
                font_info = f"{issue.font_name} ({issue.font_size:.1f}pt)"
                group['font_info'].add(font_info)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –≥—Ä—É–ø–ø—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö –∏–ª–∏ –º–Ω–æ–≥–æ —Ä–∞–∑
        filtered_groups = {}
        for text_key, data in text_groups.items():
            if len(data['pages']) >= 2 or len(data['issues']) >= 3:  # –£–º–µ–Ω—å—à–∏–ª –ø–æ—Ä–æ–≥ —Å 5 –¥–æ 3
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                data['pages_sorted'] = sorted(data['pages'])
                data['total_pages'] = len(data['pages'])
                data['total_issues'] = len(data['issues'])
                filtered_groups[text_key] = data

        return filtered_groups

    def generate_text_pattern_report(self, issues_by_type: Dict[str, List[AccessibilityIssue]]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º"""

        report = "\nüìã –ì–†–£–ü–ü–ò–†–û–í–ö–ê –ü–û –ü–û–í–¢–û–†–Ø–Æ–©–ò–ú–°–Ø –¢–ï–ö–°–¢–ê–ú:\n"
        report += "=" * 80 + "\n"

        all_pattern_reports = []

        for issue_type, issues in issues_by_type.items():
            if not issues:
                continue

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—ã —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
            text_groups = self.group_issues_by_text_pattern(issues)

            if not text_groups:
                continue

            type_report = f"\nüîç {issue_type.upper()} - –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ–∫—Å—Ç—ã:\n"
            type_report += "-" * 60 + "\n"

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –ø—Ä–æ–±–ª–µ–º
            sorted_groups = sorted(
                text_groups.items(),
                key=lambda x: (len(x[1]['pages']), len(x[1]['issues'])),
                reverse=True
            )

            for i, (text_pattern, data) in enumerate(sorted_groups[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10
                type_report += f"\n{i}. –¢–µ–∫—Å—Ç: \"{text_pattern}\"\n"

                # –°—Ç—Ä–∞–Ω–∏—Ü—ã, –≥–¥–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
                pages_list = data['pages_sorted']
                if len(pages_list) <= 10:
                    pages_str = ", ".join(str(p) for p in pages_list)
                else:
                    pages_str = f"{pages_list[0]}-{pages_list[-1]} (–≤—Å–µ–≥–æ {len(pages_list)} —Å—Ç—Ä–∞–Ω–∏—Ü)"

                type_report += f"   üìÑ –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö: {pages_str}\n"
                type_report += f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {data['total_issues']} –º–µ—Å—Ç, {data['total_words']:,} —Å–ª–æ–≤\n"

                # –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å
                severity_info = []
                for severity, count in data['severities'].items():
                    icon = 'üî¥' if severity == 'high' else ('üü°' if severity == 'medium' else 'üü¢')
                    severity_info.append(f"{icon}{count}")

                if severity_info:
                    type_report += f"   ‚ö†Ô∏è  –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {' '.join(severity_info)}\n"

                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —à—Ä–∏—Ñ—Ç–∞—Ö
                if data['font_info']:
                    fonts = list(data['font_info'])[:3]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ 3 —à—Ä–∏—Ñ—Ç–æ–≤
                    if len(fonts) == 1:
                        type_report += f"   üî§ –®—Ä–∏—Ñ—Ç: {fonts[0]}\n"
                    else:
                        type_report += f"   üî§ –®—Ä–∏—Ñ—Ç—ã: {', '.join(fonts)}\n"

                # –¢–∏–ø–∏—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
                if data['descriptions']:
                    # –ë–µ—Ä–µ–º —Å–∞–º–æ–µ —á–∞—Å—Ç–æ–µ –∏–ª–∏ –ø–µ—Ä–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                    desc = list(data['descriptions'])[0]
                    if len(desc) > 100:
                        desc = desc[:97] + "..."
                    type_report += f"   üìù –ü—Ä–æ–±–ª–µ–º–∞: {desc}\n"

            all_pattern_reports.append(type_report)

        if not all_pattern_reports:
            report += "\n‚ö†Ô∏è  –ü–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n"
        else:
            report += "\n".join(all_pattern_reports)

        return report

    def generate_color_report_improved(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–º —Ü–≤–µ—Ç–∞–º —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π"""
        if not self.color_issues:
            return ""

        report = "\nüé® –û–¢–ß–ï–¢ –ü–û –ü–†–û–ë–õ–ï–ú–ù–´–ú –¶–í–ï–¢–ê–ú:\n"
        report += "=" * 80 + "\n\n"

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ü–≤–µ—Ç–∞–º –∏ —Ç–µ–∫—Å—Ç—É
        color_text_groups = defaultdict(lambda: defaultdict(lambda: {
            'pages': defaultdict(int),
            'total_count': 0,
            'issues': [],
            'contrasts': []
        }))

        for issue in self.color_issues:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
            if 'full_text' in issue:
                normalized_text = self.normalize_text_for_grouping(issue['full_text'])
            elif 'text_sample' in issue:
                normalized_text = self.normalize_text_for_grouping(issue['text_sample'])
            else:
                continue

            if len(normalized_text) < 5:
                continue

            color_name = issue['color_name']
            color_text_groups[color_name][normalized_text]['pages'][issue['page']] += 1
            color_text_groups[color_name][normalized_text]['total_count'] += 1
            color_text_groups[color_name][normalized_text]['issues'].append(issue)
            color_text_groups[color_name][normalized_text]['contrasts'].append(issue['contrast'])

        for color_name, text_groups in sorted(color_text_groups.items()):
            total_issues = sum(len(data['issues']) for data in text_groups.values())
            total_texts = len(text_groups)

            report += f"\n{color_name.upper()} (–≤—Å–µ–≥–æ {total_issues} —Å–ª—É—á–∞–µ–≤, {total_texts} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤):\n"
            report += "-" * 60 + "\n"

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã –ø–æ —á–∞—Å—Ç–æ—Ç–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
            sorted_texts = sorted(
                text_groups.items(),
                key=lambda x: (x[1]['total_count'], len(x[1]['pages'])),
                reverse=True
            )[:10]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-10

            for i, (text, data) in enumerate(sorted_texts, 1):
                # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                text_preview = text[:60] + ("..." if len(text) > 60 else "")

                # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
                pages_list = list(data['pages'].keys())
                if len(pages_list) <= 5:
                    pages_str = ", ".join(str(p) for p in sorted(pages_list))
                    pages_info = f"–Ω–∞ —Å—Ç—Ä. {pages_str}"
                else:
                    pages_info = f"–Ω–∞ {len(pages_list)} —Å—Ç—Ä. (–ø–µ—Ä–≤–∞—è: —Å—Ç—Ä. {sorted(pages_list)[0]})"

                # –°—Ä–µ–¥–Ω—è—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å
                avg_contrast = sum(data['contrasts']) / len(data['contrasts'])

                report += f"\n{i}. –¢–µ–∫—Å—Ç: \"{text_preview}\"\n"
                report += f"   üìä –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è: {data['total_count']} —Ä–∞–∑ {pages_info}\n"
                report += f"   üé® –°—Ä–µ–¥–Ω—è—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å: {avg_contrast:.1f}:1"

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
                below_45 = sum(1 for c in data['contrasts'] if c < 4.5)

                if below_45 > 0:
                    percentage = (below_45 / len(data['contrasts'])) * 100
                    report += f" (–Ω–∏–∂–µ 4.5:1 - {below_45} —Å–ª—É—á–∞–µ–≤, {percentage:.0f}%)"

                report += "\n"

                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–µ—Ä–≤–æ–º —ç–∫–∑–µ–º–ø–ª—è—Ä–µ
                if data['issues']:
                    first_issue = data['issues'][0]
                    if 'font_size' in first_issue:
                        report += f"   üìè –†–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞: {first_issue['font_size']:.1f}pt"
                        if first_issue.get('is_large', False):
                            report += " (–∫—Ä—É–ø–Ω—ã–π —Ç–µ–∫—Å—Ç)"
                        report += "\n"

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ü–≤–µ—Ç–∞–º
        report += "\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ –¶–í–ï–¢–û–í:\n"
        report += "-" * 60 + "\n"
        report += "1. –ó–µ–ª–µ–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –±–µ–ª–æ–º —Ñ–æ–Ω–µ:\n"
        report += "   ‚Ä¢ –ü—Ä–æ–±–ª–µ–º–∞: —Å–≤–µ—Ç–ª–æ-–∑–µ–ª–µ–Ω—ã–π, —Å–∞–ª–∞—Ç–æ–≤—ã–π (–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å ~2.9-3.5:1)\n"
        report += "   ‚Ä¢ –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ–º–Ω–æ-–∑–µ–ª–µ–Ω—ã–π (#006400, #228B22)\n"
        report += "   ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç: –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å ~6.5:1 ‚úì\n\n"

        report += "2. –°–µ—Ä—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –±–µ–ª–æ–º —Ñ–æ–Ω–µ:\n"
        report += "   ‚Ä¢ –ü—Ä–æ–±–ª–µ–º–∞: —Å—Ä–µ–¥–Ω–µ-—Å–µ—Ä—ã–π (–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å ~3.9:1)\n"
        report += "   ‚Ä¢ –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ–º–Ω–æ-—Å–µ—Ä—ã–π (#333333) –∏–ª–∏ —á–µ—Ä–Ω—ã–π (#000000)\n"
        report += "   ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç: –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å 12.6:1 –∏–ª–∏ 21:1 ‚úì\n\n"

        report += "3. –ñ–µ–ª—Ç—ã–π/–æ—Ä–∞–Ω–∂–µ–≤—ã–π —Ç–µ–∫—Å—Ç:\n"
        report += "   ‚Ä¢ –ü—Ä–æ–±–ª–µ–º–∞: —è—Ä–∫–∏–π –∂–µ–ª—Ç—ã–π/–æ—Ä–∞–Ω–∂–µ–≤—ã–π (–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å ~3.0:1)\n"
        report += "   ‚Ä¢ –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–µ–º–Ω—ã–µ –æ—Ç—Ç–µ–Ω–∫–∏ –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —á–µ—Ä–Ω—ã–π\n\n"

        report += "4. –õ—É—á—à–∏–µ —Å–æ—á–µ—Ç–∞–Ω–∏—è –¥–ª—è –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏:\n"
        report += "   ‚Ä¢ –ß–µ—Ä–Ω—ã–π (#000000) –Ω–∞ –±–µ–ª–æ–º: 21:1 ‚úì\n"
        report += "   ‚Ä¢ –¢–µ–º–Ω–æ-—Å–µ—Ä—ã–π (#333333) –Ω–∞ –±–µ–ª–æ–º: 12.6:1 ‚úì\n"
        report += "   ‚Ä¢ –¢–µ–º–Ω–æ-—Å–∏–Ω–∏–π (#000066) –Ω–∞ –±–µ–ª–æ–º: 8.6:1 ‚úì\n"
        report += "   ‚Ä¢ –¢–µ–º–Ω–æ-–∑–µ–ª–µ–Ω—ã–π (#006400) –Ω–∞ –±–µ–ª–æ–º: 6.5:1 ‚úì\n"

        return report

    def generate_summary_table(self, issues: List[AccessibilityIssue]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –ø—Ä–æ–±–ª–µ–º"""
        if not issues:
            return ""

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—ã
        text_groups = self.group_issues_by_text_and_page(issues)

        report = "\nüìã –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –ü–†–û–ë–õ–ï–ú (–ì–†–£–ü–ü–ò–†–û–í–ö–ê –ü–û –¢–ï–ö–°–¢–£):\n"
        report += "=" * 80 + "\n\n"
        report += "‚Ññ | –¢–µ–∫—Å—Ç | –ü—Ä–æ–±–ª–µ–º–∞ | –°—Ç—Ä–∞–Ω–∏—Ü—ã | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å\n"
        report += "-" * 80 + "\n"

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
        sorted_groups = sorted(
            text_groups.items(),
            key=lambda x: x[1]['total_count'],
            reverse=True
        )[:50]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-50

        for i, (text, data) in enumerate(sorted_groups, 1):
            first_issue = data['first_issue']
            if not first_issue:
                continue

            # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç
            text_preview = text[:40] + ("..." if len(text) > 40 else "")

            # –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            description = list(data['descriptions'])[0] if data['descriptions'] else ""
            desc_preview = description[:50] + ("..." if len(description) > 50 else "")

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
            pages_list = list(data['pages'].keys())
            if len(pages_list) <= 3:
                pages_str = ", ".join(str(p) for p in sorted(pages_list))
            else:
                pages_str = f"{pages_list[0]}, ..., {pages_list[-1]} ({len(pages_list)} —Å—Ç—Ä.)"

            # –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å —Å –∏–∫–æ–Ω–∫–æ–π
            severity_icon = 'üî¥' if first_issue.severity == 'high' else (
                'üü°' if first_issue.severity == 'medium' else 'üü¢')

            report += f"{i:2d} | {text_preview:42s} | {desc_preview:48s} | {pages_str:15s} | {data['total_count']:4d} —Ä–∞–∑ | {severity_icon} {first_issue.severity}\n"

        return report

    def analyze(self) -> List[AccessibilityIssue]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ PDF"""
        print(f"üîç –ù–∞—á–∏–Ω–∞—é —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ PDF: {self.pdf_path}")

        try:
            with pdfplumber.open(self.pdf_path) as pdf:
                total_pages = len(pdf.pages)
                print(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")

                for page_num, page in enumerate(pdf.pages, 1):
                    print(f"  –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}/{total_pages}...", end='\r')

                    # –û—á–∏—â–∞–µ–º –∫—ç—à –¥–ª—è –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    self.line_cache.clear()
                    self.full_text_cache.clear()

                    page_issues = self.analyze_page(page_num, page)
                    self.issues.extend(page_issues)

                print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º: {len(self.issues)}")

        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ PDF: {e}")
            import traceback
            traceback.print_exc()

        return self.issues

    def generate_summary_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç —Å –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        summary = self.group_and_summarize_issues_improved()
        
        report = "üìä –ö–†–ê–¢–ö–ò–ô –û–¢–ß–ï–¢ –ü–û –î–û–°–¢–£–ü–ù–û–°–¢–ò PDF\n"
        report += "=" * 60 + "\n\n"
        report += f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {os.path.basename(self.pdf_path)}\n"
        report += f"üìà –í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {summary['overall']['total_places']:,} –º–µ—Å—Ç\n"
        report += f"üìä –í—Å–µ–≥–æ —Å–ª–æ–≤ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {summary['overall']['total_words']:,}\n"
        report += f"üìë –ó–∞—Ç—Ä–æ–Ω—É—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(summary['overall']['pages_with_issues'])}\n\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
        report += "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –°–ï–†–¨–ï–ó–ù–û–°–¢–ò:\n"
        report += "-" * 40 + "\n"
        for severity in ['high', 'medium', 'low']:
            if severity in summary['by_severity']:
                group = summary['by_severity'][severity]
                icon = 'üî¥' if severity == 'high' else ('üü°' if severity == 'medium' else 'üü¢')
                report += f"{icon} {severity.upper()}: {group['places']:,} –º–µ—Å—Ç ({group['words']:,} —Å–ª–æ–≤)\n"
        
        # –¢–æ–ø-3 —Ç–∏–ø–∞ –ø—Ä–æ–±–ª–µ–º
        report += "\nüìã –û–°–ù–û–í–ù–´–ï –¢–ò–ü–´ –ü–†–û–ë–õ–ï–ú:\n"
        report += "-" * 40 + "\n"
        type_items = sorted(
            summary['by_type'].items(),
            key=lambda x: x[1]['total_places'],
            reverse=True
        )[:3]
        
        for issue_type, type_data in type_items:
            report += f"‚Ä¢ {issue_type}: {type_data['total_places']:,} –º–µ—Å—Ç ({type_data['total_words']:,} —Å–ª–æ–≤)\n"
        
        return report

    def generate_json_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON"""
        summary = self.group_and_summarize_issues_improved()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º issues –≤ —Å–ª–æ–≤–∞—Ä–∏
        issues_dict = [asdict(issue) for issue in self.issues]
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON
        def convert_sets(obj):
            if isinstance(obj, set):
                return sorted(list(obj))
            elif isinstance(obj, dict):
                return {k: convert_sets(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_sets(item) for item in obj]
            return obj
        
        convert_sets(summary)  # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        report = {
            'document': os.path.basename(self.pdf_path),
            'document_path': self.pdf_path,
            'analysis_date': datetime.now().isoformat(),
            'summary': {
                'total_issues': summary['overall']['total_places'],
                'total_words': summary['overall']['total_words'],
                'pages_affected': sorted(list(summary['overall']['pages_with_issues'])),
                'by_severity': {
                    sev: {
                        'places': data['places'],
                        'words': data['words']
                    }
                    for sev, data in summary['by_severity'].items()
                },
                'by_type': {
                    issue_type: {
                        'total_places': data['total_places'],
                        'total_words': data['total_words'],
                        'pages_affected': sorted(list(data['pages_affected']))
                    }
                    for issue_type, data in summary['by_type'].items()
                }
            },
            'issues': issues_dict[:1000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è JSON (–ø–µ—Ä–≤—ã–µ 1000)
            'color_issues': self.color_issues[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ü–≤–µ—Ç–æ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
        }
        
        return report

    def generate_statistics_only_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç —Ç–æ–ª—å–∫–æ —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –±–µ–∑ –¥–µ—Ç–∞–ª–µ–π"""
        summary = self.group_and_summarize_issues_improved()
        
        report = "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–û–°–¢–£–ü–ù–û–°–¢–ò PDF\n"
        report += "=" * 60 + "\n\n"
        report += f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {os.path.basename(self.pdf_path)}\n"
        report += f"üìà –í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {summary['overall']['total_places']:,} –º–µ—Å—Ç\n"
        report += f"üìä –í—Å–µ–≥–æ —Å–ª–æ–≤ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {summary['overall']['total_words']:,}\n"
        report += f"üìë –ó–∞—Ç—Ä–æ–Ω—É—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(summary['overall']['pages_with_issues'])}\n\n"
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
        report += "üìã –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –¢–ò–ü–ê–ú –ò –°–ï–†–¨–ï–ó–ù–û–°–¢–ò:\n"
        report += "-" * 60 + "\n"
        
        for issue_type, severity_data in sorted(
            summary['by_type_severity'].items(),
            key=lambda x: sum(v['places'] for v in x[1].values()),
            reverse=True
        ):
            total = sum(v['places'] for v in severity_data.values())
            if total == 0:
                continue
            
            report += f"\n{issue_type}:\n"
            for severity in ['high', 'medium', 'low']:
                if severity_data[severity]['places'] > 0:
                    icon = 'üî¥' if severity == 'high' else ('üü°' if severity == 'medium' else 'üü¢')
                    places = severity_data[severity]['places']
                    pct = (places / total) * 100
                    report += f"  {icon} {severity.capitalize()}: {places:,} –º–µ—Å—Ç ({pct:.1f}%)\n"
        
        return report

    def generate_improved_report(self, output_file: str = None,
                                 create_screenshots: bool = False,
                                 screenshot_mode: str = "smart",
                                 report_format: str = "full") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å —á–µ—Ç–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –ø–æ —Ç–∏–ø–∞–º –∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏

        Args:
            output_file: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞
            create_screenshots: —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
            screenshot_mode: —Ä–µ–∂–∏–º —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤:
                - "none": –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
                - "area": —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π
                - "full_page": —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç–∞–Ω–∏—á–Ω—ã–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
                - "smart": –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º
            report_format: —Ñ–æ—Ä–º–∞—Ç –æ—Ç—á–µ—Ç–∞:
                - "full": –ø–æ–ª–Ω—ã–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                - "summary": –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç
                - "statistics": —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                - "json": JSON —Ñ–æ—Ä–º–∞—Ç
        """
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç—á–µ—Ç–∞
        if report_format == "summary":
            report = self.generate_summary_report()
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report)
                print(f"\nüìÅ –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_file}")
            print("\n" + "=" * 60)
            print(report)
            return report
        
        elif report_format == "statistics":
            report = self.generate_statistics_only_report()
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report)
                print(f"\nüìÅ –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_file}")
            print("\n" + "=" * 60)
            print(report)
            return report
        
        elif report_format == "json":
            report_dict = self.generate_json_report()
            report_json = json.dumps(report_dict, ensure_ascii=False, indent=2)
            if output_file:
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(report_json)
                print(f"\nüìÅ JSON –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_file}")
            else:
                print(report_json)
            return report_json
        
        # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø–æ–ª–Ω—ã–º –æ—Ç—á–µ—Ç–æ–º (report_format == "full")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
        summary = self.group_and_summarize_issues_improved()

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Å—Ç–∞
        unique_locations = set()
        for issue in self.issues:
            location_key = f"{issue.page}_{issue.x:.1f}_{issue.y:.1f}_{self.remove_duplicate_chars(issue.text[:50])}"
            unique_locations.add(location_key)

        report = "üìä –£–õ–£–ß–®–ï–ù–ù–´–ô –û–¢–ß–ï–¢ –ü–û –î–û–°–¢–£–ü–ù–û–°–¢–ò PDF\n"
        report += "=" * 80 + "\n\n"
        report += f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {self.pdf_path}\n"
        report += f"üìà –í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {summary['overall']['total_places']:,} –º–µ—Å—Ç\n"
        report += f"üìä –í—Å–µ–≥–æ —Å–ª–æ–≤ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {summary['overall']['total_words']:,}\n"
        report += f"üìë –ó–∞—Ç—Ä–æ–Ω—É—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(summary['overall']['pages_with_issues'])}\n"

        if not self.issues:
            report += "\n‚úÖ –ü—Ä–æ–±–ª–µ–º —Å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å—é –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!\n"
            report += "–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º WCAG 2.1.\n"
        else:
            # ==================== –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================
            report += "\nüìä –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n"
            report += "=" * 40 + "\n"

            for severity in ['high', 'medium', 'low']:
                if severity in summary['by_severity']:
                    group = summary['by_severity'][severity]
                    icon = 'üî¥' if severity == 'high' else ('üü°' if severity == 'medium' else 'üü¢')

                    report += f"\n{icon} {severity.upper()}: {group['places']:,} –º–µ—Å—Ç ({group['words']:,} —Å–ª–æ–≤)\n"

                    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –≤–Ω—É—Ç—Ä–∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
                    if group['types']:
                        for issue_type, type_data in sorted(group['types'].items(),
                                                            key=lambda x: x[1]['places'],
                                                            reverse=True):
                            report += f"   ‚Ä¢ {issue_type}: {type_data['places']:,} –º–µ—Å—Ç ({type_data['words']:,} —Å–ª–æ–≤)\n"

            # ==================== –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –¢–ò–ü–ê–ú ====================
            report += "\n\nüìã –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –¢–ò–ü–ê–ú –ü–†–û–ë–õ–ï–ú (—Å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏):\n"
            report += "=" * 60 + "\n"

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–µ—Å—Ç
            type_items = sorted(
                summary['by_type_severity'].items(),
                key=lambda x: (
                    sum(v['places'] for v in x[1].values()),  # –≤—Å–µ–≥–æ –º–µ—Å—Ç
                    sum(v['words'] for v in x[1].values())  # –≤—Å–µ–≥–æ —Å–ª–æ–≤
                ),
                reverse=True
            )

            for issue_type, severity_data in type_items:
                # –°—É–º–º–∞—Ä–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø—É
                total_places = sum(v['places'] for v in severity_data.values())
                total_words = sum(v['words'] for v in severity_data.values())

                if total_places == 0:
                    continue

                report += f"\n{issue_type.upper()}:\n"
                report += f"  –í—Å–µ–≥–æ: {total_places:,} –º–µ—Å—Ç ({total_words:,} —Å–ª–æ–≤)\n"

                # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
                for severity in ['high', 'medium', 'low']:
                    if severity_data[severity]['places'] > 0:
                        icon = 'üî¥' if severity == 'high' else ('üü°' if severity == 'medium' else 'üü¢')
                        places = severity_data[severity]['places']
                        words = severity_data[severity]['words']
                        percentage = (places / total_places) * 100 if total_places > 0 else 0

                        report += f"  {icon} {severity.capitalize()}: {places:,} –º–µ—Å—Ç ({words:,} —Å–ª–æ–≤, {percentage:.1f}%)\n"

            # ==================== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ñ–î–û–ì–û –¢–ò–ü–ê ====================
            report += "\n\nüîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û –¢–ò–ü–ê–ú –ü–†–û–ë–õ–ï–ú:\n"
            report += "=" * 60 + "\n"

            issues_by_type = defaultdict(list)
            for issue in self.issues:
                issues_by_type[issue.issue_type].append(issue)

            for issue_type, type_issues in sorted(issues_by_type.items(),
                                                  key=lambda x: len(x[1]),
                                                  reverse=True):
                if len(type_issues) < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–¥–∫–∏–µ —Ç–∏–ø—ã
                    continue

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —ç—Ç–æ–º—É —Ç–∏–ø—É
                type_summary = summary['by_type'][issue_type]

                report += f"\n{issue_type.upper()} ({type_summary['total_places']:,} –º–µ—Å—Ç):\n"
                report += "-" * 40 + "\n"

                # –î–ª—è –ø—Ä–æ–±–ª–µ–º —Å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å—é –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫—É
                if issue_type == '–ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å':
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
                    text_groups = self.group_issues_by_text_and_page(type_issues)

                    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏
                    sorted_groups = sorted(
                        text_groups.items(),
                        key=lambda x: x[1]['total_count'],
                        reverse=True
                    )[:20]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö

                    for i, (text, group_data) in enumerate(sorted_groups, 1):
                        first_issue = group_data['first_issue']
                        total_count = group_data['total_count']
                        pages_count = len(group_data['pages'])

                        # –ë–µ—Ä–µ–º –ø—Ä–∏–º–µ—Ä –æ–ø–∏—Å–∞–Ω–∏—è
                        description = list(group_data['descriptions'])[0] if group_data['descriptions'] else ""

                        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
                        if pages_count <= 5:
                            pages_list = list(group_data['pages'].keys())
                            pages_str = ", ".join(str(p) for p in sorted(pages_list))
                            pages_info = f"–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö: {pages_str}"
                        else:
                            pages_list = sorted(group_data['pages'].keys())
                            pages_info = f"–Ω–∞ {pages_count} —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö (–ø–µ—Ä–≤–∞—è: —Å—Ç—Ä. {pages_list[0]})"

                        # –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        text_preview = text[:80] + ("..." if len(text) > 80 else "")

                        report += f"\n{i}. –¢–µ–∫—Å—Ç: \"{text_preview}\"\n"
                        report += f"   üìù –ü—Ä–æ–±–ª–µ–º–∞: {description[:100]}\n"
                        report += f"   üìä –í—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è: {total_count} —Ä–∞–∑ {pages_info}\n"

                        if first_issue:
                            icon = 'üî¥' if first_issue.severity == 'high' else (
                                'üü°' if first_issue.severity == 'medium' else 'üü¢')
                            report += f"   {icon} –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {first_issue.severity}\n"
                            if first_issue.font_name:
                                report += f"   üî§ –®—Ä–∏—Ñ—Ç: {first_issue.font_name} ({first_issue.font_size:.1f}pt)\n"
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ –ø—Ä–æ–±–ª–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
                    report += f"üìÑ –ó–∞—Ç—Ä–æ–Ω—É—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(type_summary['pages_affected'])}\n"

                    # –¢–æ–ø-5 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–æ–±–ª–µ–º
                    page_counts = defaultdict(int)
                    for issue in type_issues:
                        page_counts[issue.page] += 1

                    if page_counts:
                        top_pages = sorted(page_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                        report += f"üìä –°–∞–º—ã–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:\n"
                        for page_num, count in top_pages:
                            report += f"   ‚Ä¢ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {count:,} –º–µ—Å—Ç\n"

                    # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–±–ª–µ–º
                    report += f"\nüîé –ü–†–ò–ú–ï–†–´ –ü–†–û–ë–õ–ï–ú:\n"

                    # –ë–µ—Ä–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å —Ä–∞–∑–Ω–æ–π —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å—é
                    examples_by_severity = {'high': [], 'medium': [], 'low': []}
                    for issue in type_issues[:50]:
                        examples_by_severity[issue.severity].append(issue)

                    examples_shown = 0
                    for severity in ['high', 'medium', 'low']:
                        for issue in examples_by_severity[severity][:2]:
                            text_preview = self.remove_duplicate_chars(issue.text)
                            text_preview = text_preview[:80] + ("..." if len(text_preview) > 80 else "")
                            icon = 'üî¥' if severity == 'high' else ('üü°' if severity == 'medium' else 'üü¢')

                            report += f"\n   {icon} {severity.capitalize()}: {text_preview}\n"
                            report += f"      üìù {issue.description[:120]}\n"
                            examples_shown += 1

                        if examples_shown >= 6:
                            break

            # ==================== –í–´–í–û–î –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú ====================
            report += "\n\nüìÑ –û–ë–ó–û–† –ü–û –°–¢–†–ê–ù–ò–¶–ê–ú (—Ç–æ–ø-10 —Å–∞–º—ã—Ö –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö):\n"
            report += "=" * 60 + "\n"

            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            page_stats = defaultdict(lambda: {
                'total_places': 0,
                'total_words': 0,
                'by_type': defaultdict(lambda: {'places': 0, 'words': 0}),
                'by_severity': defaultdict(int)
            })

            for issue in self.issues:
                page = page_stats[issue.page]
                word_count = self.count_words(issue.text)
                page['total_places'] += 1
                page['total_words'] += word_count
                page['by_type'][issue.issue_type]['places'] += 1
                page['by_type'][issue.issue_type]['words'] += word_count
                page['by_severity'][issue.severity] += 1

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–æ–±–ª–µ–º
            sorted_pages = sorted(
                page_stats.items(),
                key=lambda x: x[1]['total_places'],
                reverse=True
            )[:10]  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-10

            for page_num, stats in sorted_pages:
                report += f"\nüìÑ –°–¢–†–ê–ù–ò–¶–ê {page_num}:\n"
                report += f"   –í—Å–µ–≥–æ: {stats['total_places']:,} –º–µ—Å—Ç ({stats['total_words']:,} —Å–ª–æ–≤)\n"

                # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
                sev_str = []
                for sev in ['high', 'medium', 'low']:
                    if sev in stats['by_severity'] and stats['by_severity'][sev] > 0:
                        icon = 'üî¥' if sev == 'high' else ('üü°' if sev == 'medium' else 'üü¢')
                        sev_str.append(f"{icon}{stats['by_severity'][sev]}")

                if sev_str:
                    report += f"   ‚ö†Ô∏è  –°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {' '.join(sev_str)}\n"

                # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –ø—Ä–æ–±–ª–µ–º
                type_items = sorted(
                    stats['by_type'].items(),
                    key=lambda x: x[1]['places'],
                    reverse=True
                )[:3]  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-3 —Ç–∏–ø–∞

                for issue_type, type_data in type_items:
                    if type_data['places'] > 0:
                        report += f"   ‚Ä¢ {issue_type}: {type_data['places']:,} –º–µ—Å—Ç ({type_data['words']:,} —Å–ª–æ–≤)\n"

        # –û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–º —Ü–≤–µ—Ç–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å—é)
        if '–ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å' in summary['by_type_severity']:
            color_report = self.generate_color_report_improved()
            if color_report:
                report += color_report

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        report += self.generate_summary_table(self.issues)

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report += "\nüí° –û–ë–©–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:\n"
        report += "=" * 60 + "\n"

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º, –∫–∞–∫–∏–µ —Ç–∏–ø—ã –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
        issue_types_present = set(summary['by_type_severity'].keys())

        if '–ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å' in issue_types_present:
            report += "1. –£–í–ï–õ–ò–ß–¨–¢–ï –ö–û–ù–¢–†–ê–°–¢–ù–û–°–¢–¨ –¢–ï–ö–°–¢–ê:\n"
            report += "   ‚Ä¢ –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å 4.5:1\n"
            report += "   ‚Ä¢ –ö—Ä—É–ø–Ω—ã–π —Ç–µ–∫—Å—Ç (‚â•18pt –∏–ª–∏ ‚â•14pt –∂–∏—Ä–Ω—ã–π): –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å 3.0:1\n"
            report += "   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: —á–µ—Ä–Ω—ã–π (#000000), —Ç–µ–º–Ω–æ-—Å–µ—Ä—ã–π (#333333), —Ç–µ–º–Ω–æ-—Å–∏–Ω–∏–π (#000066)\n\n"

        if '–†–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞' in issue_types_present:
            report += "2. –£–í–ï–õ–ò–ß–¨–¢–ï –†–ê–ó–ú–ï–† –®–†–ò–§–¢–ê:\n"
            report += "   ‚Ä¢ –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 12pt (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 14-16pt)\n"
            report += "   ‚Ä¢ –ó–∞–≥–æ–ª–æ–≤–∫–∏: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 14pt (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16-18pt)\n"
            report += "   ‚Ä¢ –î–ª—è —Å–ª–∞–±–æ–≤–∏–¥—è—â–∏—Ö: –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç 16-18pt, –∑–∞–≥–æ–ª–æ–≤–∫–∏ 20-24pt\n\n"

        if '–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å —à—Ä–∏—Ñ—Ç–∞' in issue_types_present:
            report += "3. –í–´–ë–ï–†–ò–¢–ï –ß–ò–¢–ê–ï–ú–´–ï –®–†–ò–§–¢–´:\n"
            report += "   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: Arial, Verdana, Tahoma, Georgia\n"
            report += "   ‚Ä¢ –ò–∑–±–µ–≥–∞–π—Ç–µ: –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö, –º–æ–Ω–æ—à–∏—Ä–∏–Ω–Ω—ã—Ö, —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —à—Ä–∏—Ñ—Ç–æ–≤\n\n"

        report += "üìã –°–¢–ê–ù–î–ê–†–¢–´ WCAG 2.1 (–£—Ä–æ–≤–µ–Ω—å AA):\n"
        report += "   ‚Ä¢ –ö–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞: 4.5:1 (3.0:1 –¥–ª—è –∫—Ä—É–ø–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞)\n"
        report += "   ‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 2.5–º–º\n"
        report += "   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ü–≤–µ—Ç–∞: –Ω–µ –ø–æ–ª–∞–≥–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ —Ü–≤–µ—Ç –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n"

        # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
        print("\n" + "=" * 80)
        print(report)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\nüìÅ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_file}")

        return report


# –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï
if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–∏–∑—É–∞–ª—å–Ω–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ PDF —Ñ–∞–π–ª–æ–≤',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python main.py document.pdf
  python main.py document.pdf --format summary
  python main.py document.pdf --format json --output report.json
  python main.py document.pdf --format statistics --screenshots
  
  python main.py  # –ó–∞–ø—É—Å–∫ –±–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞–∂–µ—Ç —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ
        """
    )
    
    # –ü—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–∞
    default_pdf_path = r"C:\Users\vikharev-d\Downloads\Telegram Desktop\2_5291784456137378049.pdf"
    
    parser.add_argument(
        'pdf_path',
        type=str,
        nargs='?',  # –î–µ–ª–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º
        default=default_pdf_path,
        help=f'–ü—É—Ç—å –∫ PDF —Ñ–∞–π–ª—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {os.path.basename(default_pdf_path)})'
    )
    
    parser.add_argument(
        '--format', '-f',
        type=str,
        choices=['full', 'summary', 'statistics', 'json'],
        default='full',
        help='–§–æ—Ä–º–∞—Ç –æ—Ç—á–µ—Ç–∞: full (–ø–æ–ª–Ω—ã–π), summary (–∫—Ä–∞—Ç–∫–∏–π), statistics (—Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞), json (JSON —Ñ–æ—Ä–º–∞—Ç)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–º—è —Ñ–∞–π–ª–∞)'
    )
    
    parser.add_argument(
        '--screenshots', '-s',
        action='store_true',
        help='–°–æ–∑–¥–∞–≤–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –æ–±–ª–∞—Å—Ç–µ–π'
    )
    
    parser.add_argument(
        '--screenshot-mode',
        type=str,
        choices=['none', 'area', 'full_page', 'smart'],
        default='smart',
        help='–†–µ–∂–∏–º —Å–æ–∑–¥–∞–Ω–∏—è —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤: none, area, full_page, smart'
    )
    
    args = parser.parse_args()
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    pdf_path = args.pdf_path
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
    if not os.path.exists(pdf_path):
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {pdf_path}")
        print(f"\nüí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É.")
        print(f"üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —á—Ç–µ–Ω–∏—è.")
        print(f"\nüí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print(f"  python main.py [–ø—É—Ç—å_–∫_pdf_—Ñ–∞–π–ª—É] [–æ–ø—Ü–∏–∏]")
        print(f"  python main.py --help  # –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤—Å–µ—Ö –æ–ø—Ü–∏–π")
        exit(1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ PDF —Ñ–∞–π–ª
    if not pdf_path.lower().endswith('.pdf'):
        print(f"‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –§–∞–π–ª –Ω–µ –∏–º–µ–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è .pdf: {pdf_path}")
        response = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∞–Ω–∞–ª–∏–∑? (y/n): ").strip().lower()
        if response != 'y':
            print("–ê–Ω–∞–ª–∏–∑ –æ—Ç–º–µ–Ω–µ–Ω.")
            exit(0)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è
    if not os.access(pdf_path, os.R_OK):
        print(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {pdf_path}")
        print(f"üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª—É.")
        exit(1)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç—á–µ—Ç–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
    if args.output is None:
        base_name = os.path.splitext(os.path.basename(pdf_path))[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if args.format == 'json':
            args.output = f"reports/{base_name}_report_{timestamp}.json"
        elif args.format == 'summary':
            args.output = f"reports/{base_name}_summary_{timestamp}.txt"
        elif args.format == 'statistics':
            args.output = f"reports/{base_name}_statistics_{timestamp}.txt"
        else:
            args.output = f"reports/{base_name}_full_report_{timestamp}.txt"
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É reports, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs('reports', exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    analyzer = EnhancedPDFAccessibilityAnalyzer(pdf_path)

    # –ü—Ä–æ–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑
    print("üöÄ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏...")
    print(f"üìÑ –§–∞–π–ª: {pdf_path}")
    print(f"üìä –§–æ—Ä–º–∞—Ç –æ—Ç—á–µ—Ç–∞: {args.format}")
    print("-" * 60)
    
    issues = analyzer.analyze()

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    report = analyzer.generate_improved_report(
        output_file=args.output,
        create_screenshots=args.screenshots,
        screenshot_mode=args.screenshot_mode if args.screenshots else "none",
        report_format=args.format
    )

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if analyzer.issues:
        summary = analyzer.group_and_summarize_issues_improved()

        print("\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print("-" * 40)

        print(f"–û–±—â–∏–π –æ–±—ä–µ–º –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {summary['overall']['total_words']:,} —Å–ª–æ–≤")

        # –ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö)
        unique_texts = set()
        for issue in analyzer.issues:
            normalized = analyzer.remove_duplicate_chars(issue.text)
            unique_texts.add(normalized[:100])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤

        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(unique_texts):,}")

        print("\n–°–∞–º—ã–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ –æ–±—ä–µ–º—É —Ç–µ–∫—Å—Ç–∞):")
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        page_words = defaultdict(int)
        page_issues = defaultdict(int)

        for issue in analyzer.issues:
            page_words[issue.page] += analyzer.count_words(issue.text)
            page_issues[issue.page] += 1

        sorted_pages = sorted(page_words.items(), key=lambda x: x[1], reverse=True)[:5]

        for page_num, words in sorted_pages:
            issues_count = page_issues[page_num]
            print(f"\n  üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}:")
            print(f"    ‚Ä¢ –í—Å–µ–≥–æ —Å–ª–æ–≤: {words:,}")
            print(f"    ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {issues_count:,}")
            if issues_count > 0:
                print(f"    ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–æ–±–ª–µ–º—É: {words / issues_count:.1f} —Å–ª–æ–≤")

    if analyzer.color_issues:
        print("\nüé® –°–í–û–î–ö–ê –ü–û –¶–í–ï–¢–ê–ú (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã):")
        color_summary = defaultdict(lambda: {'count': 0, 'unique_texts': set(), 'words': 0})
        for issue in analyzer.color_issues:
            color = issue['color_name']
            color_summary[color]['count'] += 1
            text = issue.get('normalized_text', '') or issue.get('full_text', '') or issue.get('text_sample', '')
            if text:
                color_summary[color]['unique_texts'].add(text[:100])
                color_summary[color]['words'] += analyzer.count_words(text)

        for color, stats in sorted(color_summary.items(), key=lambda x: x[1]['words'], reverse=True):
            print(
                f"  {color}: {stats['count']:,} —Å–ª—É—á–∞–µ–≤, {len(stats['unique_texts']):,} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, {stats['words']:,} —Å–ª–æ–≤")